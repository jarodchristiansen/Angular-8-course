{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of O'Reilly ML Engineer Take home",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jarodchristiansen/Angular-8-course/blob/master/Copy_of_O'Reilly_ML_Engineer_Take_home.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw38KAbDKSOm"
      },
      "source": [
        "# O'Reilly Machine Learning Engineer Takehome\n",
        "\n",
        "Welcome to the evaluation project for the Machine Learning Engineer position at O'Reilly Media. In this project you will evaluate a search academic dataset built using common learn-to-rank features, build a ranking model using the dataset, and discuss how additional features could be used and how they would impact the performance of the model.\n",
        "\n",
        "Steps:\n",
        "1. Make a copy of this notebook.\n",
        "2. Download the dataset to the notebook\n",
        "3. Preprocess and evaluate the dataset\n",
        "4. Build a **ranking** model\n",
        "5. Evaluate your ranking model using a metric of your choice\n",
        "6. Discuss the performance of your model and why you chose the model you chose.\n",
        "7. Answer discussion questions\n",
        "8. Submit your notebook\n",
        "\n",
        "\n",
        "## Notes\n",
        "\n",
        " \n",
        "\n",
        "*   Throughout the notebook you should include notes explaining your choices and what you are doing. Your thought process is more important than the actual performance of your model.\n",
        "*   Create as many cells as you want. The exisiting cells are just provided to provide some initial organization.\n",
        "* You may use any choice of libraries or frameworks.\n",
        "* If ranking models are new to you, consider starting here: https://arxiv.org/abs/1812.00073"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQbF5lfeQ7B-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180bb129-4ac2-40a2-9a00-d8abd67b335b"
      },
      "source": [
        "# Import dependencies here\n",
        "!pip install pyltr\n",
        "import pyltr"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyltr\n",
            "  Downloading https://files.pythonhosted.org/packages/29/01/e7120dffc8bb40307002a51b85810ef714ee3faed260c416e9ae38feb282/pyltr-0.2.6-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pyltr) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyltr) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyltr) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pyltr) (1.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyltr) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyltr) (0.17.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pyltr) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pyltr) (2.8.1)\n",
            "Installing collected packages: pyltr\n",
            "Successfully installed pyltr-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1am1Iv_LWR2W"
      },
      "source": [
        "### 2) Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_4tmowxOHAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7951f639-62cd-42bd-9c59-cc8e478bff38"
      },
      "source": [
        "# Download the dataset located at https://storage.googleapis.com/personalization-takehome/MSLR-WEB10K.zip\n",
        "# You can read about the features included in the dataset here: https://www.microsoft.com/en-us/research/project/mslr/\n",
        "!wget https://storage.googleapis.com/personalization-takehome/MSLR-WEB10K.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-22 15:11:02--  https://storage.googleapis.com/personalization-takehome/MSLR-WEB10K.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.26.128, 172.217.193.128, 172.217.204.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.26.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1234144912 (1.1G) [application/zip]\n",
            "Saving to: ‘MSLR-WEB10K.zip’\n",
            "\n",
            "MSLR-WEB10K.zip     100%[===================>]   1.15G   124MB/s    in 10s     \n",
            "\n",
            "2020-11-22 15:11:13 (113 MB/s) - ‘MSLR-WEB10K.zip’ saved [1234144912/1234144912]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bqHXeWE3-5o",
        "outputId": "7017673d-3ab7-4c6c-d220-8d252b437164"
      },
      "source": [
        "!unzip MSLR-WEB10K.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  MSLR-WEB10K.zip\n",
            "   creating: Fold1/\n",
            "  inflating: Fold1/test.txt          \n",
            "  inflating: Fold1/train.txt         \n",
            "  inflating: Fold1/vali.txt          \n",
            "   creating: Fold2/\n",
            "  inflating: Fold2/test.txt          \n",
            "  inflating: Fold2/train.txt         \n",
            "  inflating: Fold2/vali.txt          \n",
            "   creating: Fold3/\n",
            "  inflating: Fold3/test.txt          \n",
            "  inflating: Fold3/train.txt         \n",
            "  inflating: Fold3/vali.txt          \n",
            "   creating: Fold4/\n",
            "  inflating: Fold4/test.txt          \n",
            "  inflating: Fold4/train.txt         \n",
            "  inflating: Fold4/vali.txt          \n",
            "   creating: Fold5/\n",
            "  inflating: Fold5/test.txt          \n",
            "  inflating: Fold5/train.txt         \n",
            "  inflating: Fold5/vali.txt          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQdKuIDNWVb8"
      },
      "source": [
        "### 3) Preprocess and evaluate the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHEqbC9sOrvb"
      },
      "source": [
        "# Preprocess and evaluate the dataset\n",
        "with open('Fold1/train.txt') as trainfile, \\\n",
        "        open('Fold1/vali.txt') as valifile, \\\n",
        "        open('Fold1/test.txt') as evalfile:\n",
        "    TX, Ty, Tqids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
        "    VX, Vy, Vqids, _ = pyltr.data.letor.read_dataset(valifile)\n",
        "    EX, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OICi0aAuWclQ"
      },
      "source": [
        "4) Build ranking model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frHrBKmTSUsq"
      },
      "source": [
        "# Build ranking model\n",
        "metric = pyltr.metrics.NDCG(k=10)\n",
        "metric2 = pyltr.metrics.ERR(highest_score=.34, k=10)\n",
        "\n",
        "# Only needed if you want to perform validation (early stopping & trimming)\n",
        "monitor = pyltr.models.monitors.ValidationMonitor(\n",
        "    VX, Vy, Vqids, metric=metric, stop_after=400)\n",
        "\n",
        "model = pyltr.models.LambdaMART(\n",
        "    metric=metric,\n",
        "    n_estimators=800,\n",
        "    learning_rate=0.02,\n",
        "    max_features=0.5,\n",
        "    query_subsample=0.5,\n",
        "    max_leaf_nodes=10,\n",
        "    min_samples_leaf=64,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "model.fit(TX, Ty, Tqids, monitor=monitor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBtVqWRSWx_g"
      },
      "source": [
        "5) Evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpaP8jesTQMj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "dcad5405-56ed-4dd1-c4a9-e85fa1dc21b5"
      },
      "source": [
        "predictions = model.predict(EX)\n",
        "metric.evaluate_preds(Vqids,Vy, predictions)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-3071c254d6b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVqids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyltr/metrics/_metrics.py\u001b[0m in \u001b[0;36mevaluate_preds\u001b[0;34m(self, qid, targets, preds)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_sorted_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_random_ev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyltr/util/sort.py\u001b[0m in \u001b[0;36mget_sorted_y\u001b[0;34m(y, y_pred, check)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_sorted_y_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyltr/util/sort.py\u001b[0m in \u001b[0;36mget_sorted_y_positions\u001b[0;34m(y, y_pred, check)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [235259, 241521]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f3RmmCUTdNp"
      },
      "source": [
        "### 6) Please answer the following questions about your choices:\n",
        "1. Why did you choose your metric to evaluate the model?\n",
        "2. How well would you say your model performed?\n",
        "3. If you had more time what else would you want to try?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYwv7d8pTn6V"
      },
      "source": [
        "### 7) Please answer the following questions about how you would use additional features:\n",
        "\n",
        "1. If you had an additional feature for each row of the dataset that was unique identifier for the user performing the query e.g. `user_id`, how could you use it to improve the performance of the model?\n",
        "2. If you had the additional features of: `query_text` or the actual textual query itself, as well as document text features like `title_text`, `body_text`, `anchor_text`, `url` for the document, how would you include them in your model (or any model) to improve its performance?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy73cbghWBM3"
      },
      "source": [
        "### 8) Please submit your colab by sharing it with: cmaon@oreilly.com\n"
      ]
    }
  ]
}